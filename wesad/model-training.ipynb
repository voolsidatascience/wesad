{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phuong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Phuong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Phuong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Phuong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Phuong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Phuong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "module = os.path.relpath('./wesad_experiments-master/src/main')\n",
    "if module not in sys.path:\n",
    "    sys.path.append(module)\n",
    "from DataManager import DataManager\n",
    "#from Demo import Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will go ahead and do the work to prepare the data for NN creation and actually create the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for S2\n",
      "Loading data for S3\n",
      "Loading data for S4\n",
      "Loading data for S5\n",
      "Loading data for S6\n",
      "Loading data for S7\n",
      "Loading data for S8\n",
      "Loading data for S9\n",
      "Loading data for S10\n",
      "Loading data for S11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "manager.load_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features..\n",
      "\tsubject: 2\n",
      "\tsubject: 3\n",
      "\tsubject: 4\n",
      "\tsubject: 5\n",
      "\tsubject: 6\n",
      "\tsubject: 7\n",
      "\tsubject: 8\n",
      "\tsubject: 9\n",
      "\tsubject: 10\n",
      "\tsubject: 11\n",
      "conputing features..\n",
      "\tsubject: 2\n",
      "\tsubject: 3\n",
      "\tsubject: 4\n",
      "\tsubject: 5\n",
      "\tsubject: 6\n",
      "\tsubject: 7\n",
      "\tsubject: 8\n",
      "\tsubject: 9\n",
      "\tsubject: 10\n",
      "\tsubject: 11\n"
     ]
    }
   ],
   "source": [
    "manager.compute_features();\n",
    "manager.compute_features_stress();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10  subjects\n",
      "there are  44461  values for  a_mean\n",
      "there are  44461  values for  a_std\n",
      "there are  44461  values for  a_maxx\n",
      "there are  44461  values for  a_maxy\n",
      "there are  44461  values for  a_maxz\n",
      "there are  44461  values for  e_max\n",
      "there are  44461  values for  e_min\n",
      "there are  44461  values for  e_mean\n",
      "there are  44461  values for  e_range\n",
      "there are  44461  values for  e_std\n",
      "there are  44461  values for  t_max\n",
      "there are  44461  values for  t_min\n",
      "there are  44461  values for  t_mean\n",
      "there are  44461  values for  t_range\n",
      "there are  44461  values for  t_std\n"
     ]
    }
   ],
   "source": [
    "print(\"We have\", len(manager.SUBJECTS), \" subjects\")\n",
    "\n",
    "for feature in manager.FEATURES.keys():\n",
    "    print(\"there are \", len(manager.FEATURES[feature]), \" values for \", feature)\n",
    "#     print(manager.FEATURES[feature][3277:3288])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape the data such that there are [samples, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44461, 15)\n",
      "(23780, 15)\n"
     ]
    }
   ],
   "source": [
    "# Lets go ahead and reshape this data such that we have\n",
    "# samples, features = [N, 15]\n",
    "\n",
    "X1 = []\n",
    "X2 = []\n",
    "for i in range(0,  len(manager.FEATURES['a_mean'])):\n",
    "    X1.append([manager.FEATURES['a_mean'][i], manager.FEATURES['a_std'][i], manager.FEATURES['a_maxx'][i], manager.FEATURES['a_maxy'][i], manager.FEATURES['a_maxz'][i],\\\n",
    "                  manager.FEATURES['e_max'][i], manager.FEATURES['e_min'][i], manager.FEATURES['e_mean'][i], manager.FEATURES['e_range'][i], manager.FEATURES['e_std'][i],\\\n",
    "                  manager.FEATURES['t_max'][i], manager.FEATURES['t_min'][i], manager.FEATURES['t_mean'][i], manager.FEATURES['t_range'][i], manager.FEATURES['t_std'][i]])\n",
    "print(np.shape(X1))\n",
    "\n",
    "for i in range(0,  len(manager.STRESS_FEATURES['a_mean'])):\n",
    "    X2.append([manager.STRESS_FEATURES['a_mean'][i], manager.STRESS_FEATURES['a_std'][i], manager.STRESS_FEATURES['a_maxx'][i], manager.STRESS_FEATURES['a_maxy'][i], manager.STRESS_FEATURES['a_maxz'][i],\\\n",
    "                  manager.STRESS_FEATURES['e_max'][i], manager.STRESS_FEATURES['e_min'][i], manager.STRESS_FEATURES['e_mean'][i], manager.STRESS_FEATURES['e_range'][i], manager.STRESS_FEATURES['e_std'][i],\\\n",
    "                  manager.STRESS_FEATURES['t_max'][i], manager.STRESS_FEATURES['t_min'][i], manager.STRESS_FEATURES['t_mean'][i], manager.STRESS_FEATURES['t_range'][i], manager.STRESS_FEATURES['t_std'][i]] )\n",
    "print(np.shape(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(manager.FEATURES['a_mean'][100:110])\n",
    "\n",
    "# print(manager.FEATURES['a_std'][400:410])\n",
    "\n",
    "# print(manager.FEATURES['a_maxx'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=8, inter_op_parallelism_threads=8, )\n",
    "\n",
    "# tf.set_random_seed(1)\n",
    "\n",
    "# session = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize two arrays for the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 is the baseline data\n",
    "# x2 is the stress data\n",
    "\n",
    "y1 = [0] * len(X1)\n",
    "y2 = [1] * len(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44461\n",
      "23780\n"
     ]
    }
   ],
   "source": [
    "print(len(y1))\n",
    "print(len(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to concat the data between baseline and stress so that we can split it into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68241, 15)\n",
      "(68241,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X1, X2), axis=0)\n",
    "print(np.shape(X))\n",
    "\n",
    "y = np.concatenate((y1,y2), axis=0)\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data up in train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# since we don't have the same number of samples for x and y, \n",
    "# we will drop off some x values for creating the sample\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51180, 15)\n",
      "(17061, 15)\n",
      "(51180,)\n",
      "(17061,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to scale the data and hope we get better results.\n",
    "1. fit the scaler to the training data (fit_transform)\n",
    "2. transform training data with the scaler\n",
    "3. fit the model with transformed data\n",
    "4. transform test data with the scaler\n",
    "5. predict using model and output of (4)\n",
    "\n",
    "It will be better to have our data scaled between 0 and 1, so lets go ahead and create a function to \n",
    "normalize the data that way now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(data)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be\n",
    "input_shape=(number of sequences=?, time_steps=None, features=15)\n",
    "target=(number of sequences, time_steps, targets)\n",
    "\n",
    "We will use time_steps = 1 since our data has implicit timesteps from the feature calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51180, 1, 15)\n",
      "(17061, 1, 15)\n",
      "(51180,)\n",
      "(17061,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 15\n",
    "num_features = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the LSTM network...\n",
      "WARNING:tensorflow:From C:\\Users\\Phuong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "print('Building the LSTM network...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=True))\n",
    "# model.add(LSTM(16, input_shape=(1, 15), dropout=0.35, recurrent_dropout=0.35, return_sequences=True))\n",
    "# Note:\n",
    "# Need to do return_sequences=False for the layer *before* dense\n",
    "# https://stackoverflow.com/questions/51763983/error-when-checking-target-expected-dense-1-to-have-3-dimensions-but-got-array\n",
    "model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 15)             1860      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 15)                1860      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 3,736\n",
      "Trainable params: 3,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "inputs:  (None, 1, 15)\n",
      "outputs:  (None, 1)\n",
      "actual inputs:  (51180, 1, 15)\n",
      "actual outputs:  (51180,)\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "print(\"inputs: \" , model.input_shape)\n",
    "print(\"outputs: \", model.output_shape)\n",
    "print(\"actual inputs: \", np.shape(X_train))\n",
    "print(\"actual outputs: \", np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.05)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM...\n",
      "WARNING:tensorflow:From C:\\Users\\Phuong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 51180 samples, validate on 17061 samples\n",
      "Epoch 1/5\n",
      "51180/51180 [==============================] - 76s 1ms/step - loss: 0.4095 - accuracy: 0.8001 - val_loss: 0.2111 - val_accuracy: 0.9178\n",
      "Epoch 2/5\n",
      "51180/51180 [==============================] - 74s 1ms/step - loss: 0.1528 - accuracy: 0.9412 - val_loss: 0.1174 - val_accuracy: 0.9572\n",
      "Epoch 3/5\n",
      "51180/51180 [==============================] - 74s 1ms/step - loss: 0.0888 - accuracy: 0.9666 - val_loss: 0.0580 - val_accuracy: 0.9797\n",
      "Epoch 4/5\n",
      "51180/51180 [==============================] - 75s 1ms/step - loss: 0.0672 - accuracy: 0.9740 - val_loss: 0.0420 - val_accuracy: 0.9844\n",
      "Epoch 5/5\n",
      "51180/51180 [==============================] - 74s 1ms/step - loss: 0.0568 - accuracy: 0.9779 - val_loss: 0.0545 - val_accuracy: 0.9735\n",
      "17061/17061 [==============================] - 7s 390us/step\n"
     ]
    }
   ],
   "source": [
    "print('Training LSTM...')\n",
    "\n",
    "batch_size = 2 # I think I want to use batch_size = 1\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.05447737499224626\n",
      "accuracy: 0.9735068082809448\n"
     ]
    }
   ],
   "source": [
    "print('score:', score)\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disc\n"
     ]
    }
   ],
   "source": [
    "# Let's save the model to disc\n",
    "json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as file:\n",
    "    file.write(json)\n",
    "    file.close()\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to disk\n"
     ]
    }
   ],
   "source": [
    "# save model and architecture to a single file\n",
    "model.save(\"best_LSTM_model.h5\")\n",
    "print(\"Model saved to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import lite\n",
    "converter = lite.TFLiteConverter.from_saved_model(\"best_LSTM_model.h5\")\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"best_LSTM_model.tflite\", \"wb\") as file:\n",
    "    file.write(tflite_model)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import lite\n",
    "converter = lite.TFLiteConverter.from_keras_model_file(\"best_LSTM_model.h5\")\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"best_LSTM_model.tflite\", \"wb\") as file:\n",
    "    file.write(tflite_model)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# Load the model of interest\n",
    "json_file = open('model.json', 'r')\n",
    "json = json_file.read()\n",
    "json_file.close()\n",
    "model_from_disc = model_from_json(json)\n",
    "model_from_disc.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34121/34121 [==============================] - 12s 354us/step\n",
      "score: 0.2466312063727222\n",
      "accuracy: 0.9004132151603699\n"
     ]
    }
   ],
   "source": [
    "opt = SGD(lr=0.05)\n",
    "model_from_disc.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "score, acc = model_from_disc.evaluate(X_test , y_test, batch_size=batch_size)\n",
    "print('score:', score)\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34121/34121 [==============================] - 10s 285us/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model_from_disc.predict(X_test, batch_size=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.2300039e-03]\n",
      " [9.8335546e-01]\n",
      " [9.6699440e-01]\n",
      " ...\n",
      " [8.5141414e-01]\n",
      " [1.7531286e-01]\n",
      " [9.4874442e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92     22308\n",
      "           1       0.78      1.00      0.87     11813\n",
      "\n",
      "    accuracy                           0.90     34121\n",
      "   macro avg       0.89      0.92      0.90     34121\n",
      "weighted avg       0.92      0.90      0.90     34121\n",
      "\n",
      "[[18963  3345]\n",
      " [   53 11760]]\n"
     ]
    }
   ],
   "source": [
    "y_pred[y_pred>0.5] = 1 \n",
    "y_pred[y_pred<=0.5] = 0 \n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance using the LSTM based network architecture with one hidden layer\n",
    "has performed with an accuracy of ~ 97.7% on the validation set using 5 epochs.\n",
    "\n",
    "With just one epoch, the model has results between ~80% and ~92% for accuracy\n",
    "on the validation data. Each epoch of training takes approximately 70 seconds\n",
    "without GPU acceleration. At 5 epochs, the model outperforms the WESAD quoated\n",
    "results for both accuracy and F1 using less modalities and less features."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
